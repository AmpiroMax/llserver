{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from llserver.utils.handler import UniserverHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = UniserverHandler(port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler.get_running_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler.start_model(\"lera_api\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = handler.get_running_models()\n",
    "model_id = list(info[\"running_models\"][\"models\"].keys())[0]\n",
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"compare images, describe the difference between them\"\n",
    "base_path = \"/llserver/data/\"\n",
    "image_paths = [\n",
    "    base_path + \"lera_test_replanner/0.png\",\n",
    "    base_path + \"metatmp.png\",\n",
    "]\n",
    "# model = \"gemini-pro-1.5\"\n",
    "model = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "put_response = handler.put_task(model_id=model_id, prompt=prompt, image_paths=image_paths, model=model)\n",
    "task_id = put_response[\"task_id\"][\"task_id\"]\n",
    "task_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = \"\"\n",
    "wait_time = 3\n",
    "while status != \"completed\":\n",
    "    time.sleep(wait_time)\n",
    "    result_response = handler.get_task_result(model_id=model_id, task_id=task_id)\n",
    "    status = result_response.get(\"status\")\n",
    "    print(f\"Status: {status}\")\n",
    "\n",
    "result = result_response.get(\"result\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler.stop_model(model_id=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler.stop_all_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Lera API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLServer and Relanner\n",
    "\n",
    "import os\n",
    "from llserver.utils.handler import UniserverHandler\n",
    "import json\n",
    "import time\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def save_images(images, dirs):\n",
    "    for idx, (image, dir) in enumerate(zip(images, dirs)):\n",
    "        Image.fromarray(image).save(dir)\n",
    "    return dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "instruction = \"Move plates, wineglasses, mugs, dishbowls from the table into the dishwasher and then turn it on.\"\n",
    "suc_actions = [\n",
    "    f'[Walk] <kitchentable> (226)',\n",
    "    f'[Walk] <bench> (227)',\n",
    "#    f'[Walk] <bananas> (1001)',\n",
    "#    f'[Grab] <bananas> (1001)',\n",
    "]\n",
    "next_actions = [\n",
    "#    f'[Walk] <kitchentable> (226)',\n",
    "#    f'[Walk] <bench> (227)',\n",
    "    f'[Walk] <bananas> (1001)',\n",
    "    f'[Grab] <bananas> (1001)',\n",
    "]\n",
    "# image_path = r\"C:\\Users\\prestige\\Desktop\\Work\\virtualhome100\\windows_sim\\Output\\script\\Action_0082_normal.png\"\n",
    "image_path = \"/home/mpatratskiy/work/meta_world/llserver/data/metatmp.png\"\n",
    "images = [np.array(Image.open(image_path))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server is ready to go\n"
     ]
    }
   ],
   "source": [
    "# Replanning initialization\n",
    "\n",
    "handler = UniserverHandler(port=8000)\n",
    "start_response = handler.start_model(\"lera_api\")\n",
    "model_id = start_response[\"model_id\"]\n",
    "\n",
    "experiment_name = \"VhomeDebug\"\n",
    "model_name = \"gpt-4o\" #+ \"###\" + \"alfred\"\n",
    "# save_base_dir = f'C:\\\\Users\\\\prestige\\\\Desktop\\\\Work\\\\virtualhome100\\\\llserver\\\\data\\\\{experiment_name}'\n",
    "# load_base_dir = f\"/llserver/data/{experiment_name}\"\n",
    "\n",
    "save_base_dir = f\"/home/mpatratskiy/work/meta_world/llserver/data/{experiment_name}\"\n",
    "load_base_dir = f\"/llserver/data/{experiment_name}\"\n",
    "os.makedirs(save_base_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/mpatratskiy/work/meta_world/llserver/data/VhomeDebug/0.png']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process input\n",
    "\n",
    "prompt = f\"\"\"{instruction}###{suc_actions}###{next_actions}###\"\"\"\n",
    "\n",
    "dirs_to_save = [f\"{save_base_dir}/{idx}.png\" for idx in range(len(images))]\n",
    "dirs_to_load = [f\"{load_base_dir}/{idx}.png\" for idx in range(len(images))]\n",
    "save_images(images, dirs_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt-4o'}\n",
      "{'model_id': 'e5bb4593-5683-49b2-8f78-f5800adfdbe3', 'prompt': \"Move plates, wineglasses, mugs, dishbowls from the table into the dishwasher and then turn it on.###['[Walk] <kitchentable> (226)', '[Walk] <bench> (227)']###['[Walk] <bananas> (1001)', '[Grab] <bananas> (1001)']###\", 'image_paths': ['/llserver/data/VhomeDebug/0.png'], 'extra_params': {'model': 'gpt-4o'}}\n"
     ]
    }
   ],
   "source": [
    "put_response = handler.put_task(model_id=model_id, prompt=prompt, image_paths=dirs_to_load, model=model_name)\n",
    "task_id = put_response[\"task_id\"][\"task_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: completed\n",
      "{'[goal]': 'Move plates, wineglasses, mugs, dishbowls from the table into the dishwasher and then turn it on.', '[success_actions]': \"['[Walk] <kitchentable> (226)', '[Walk] <bench> (227)']\", '[current_plan]': \"['[Walk] <bananas> (1001)', '[Grab] <bananas> (1001)']\", '[available_actions]': '', '[look_response]': \"The scene shows a robot arm positioned over a table. Here's a detailed description:\\n\\n- **Robot Arm**: The robot arm is red and appears to be mid-action over the table. It is extended, with its gripper hovering just above the table surface.\\n  \\n- **Gripper**: The gripper has its pincers open, positioned to either grab or release an object. It is currently empty, indicating that it may have accidentally dropped an object during movement.\\n\\n- **Objects on the Table**: \\n  - A red cylinder lies directly under the gripper, suggesting it might have been dropped.\\n  - A blue sphere is located slightly to the right of the gripper.\\n  - The rest of the table appears to be flat and without other objects.\\n\\nGiven the current goal of moving specific kitchenware into a dishwasher, it appears that the robot's current task is not directly aligned with the goal, as it seems to be interacting with potentially unrelated objects (cylinder and sphere) rather than plates, wineglasses, mugs, or dishbowls.\", '[explain_response]': \"Given the current situation, it seems the robot has been interacting with objects unrelated to the goal of moving kitchenware to the dishwasher. The robot may have unintentionally dropped an object and is not currently aligned with its primary task. Let's devise a new plan to address the problem and complete the goal. Hereâ€™s how we can proceed:\\n\\n1. **Locate Relevant Kitchenware**: First, we need to identify any plates, wineglasses, mugs, or dishbowls present on the table. This step is essential to align the robot's actions with the goal. This would be using the `locate('object')` action for each relevant kitchenware item.\\n\\n2. **Pick Kitchenware**: After locating each piece of kitchenware, the robot should pick them up using `pick('object')`.\\n\\n3. **Transport Kitchenware to Dishwasher**: Although this transportation part isn't defined in the available actions, we assume that relocating the items near the dishwasher involves having them picked correctly first.\\n\\n4. **Place Kitchenware in Dishwasher**: Once at the dishwasher location, we should use `place_on_top_of('object')` to ensure kitchenware is placed correctly, possibly on designated slots within the dishwasher or positioning to imitate an unloading scenario through the action mechanics.\\n\\n5. **Done**: Once all items are in the dishwasher, use `done()` to indicate task completion.\\n\\nSince there are discrepancies between current actions and goal targeting, let's create a direct action plan based on available actions to address this:\\n\\n### New Plan:\\n\\n1. **Locate and Pick Each Relevant Item**:\\n   - `locate('plate')` - Locate a plate and pick it up, if available.\\n   - `pick('plate')`\\n   - `place_on_top_of('dishwasher')` - This assumes the robot places each located and picked kitchenware item on top of or in a part of the dishwasher until further guidance.\\n   \\n   Repeat the locate, pick, and place sequence for **wineglasses**, **mugs**, and **dishbowls** if they are present.\\n\\n2. **Completion**:\\n   - After ensuring the appropriate objects are moved as per the task specification, use `done()` to signal task completion.\\n\\n### Explanation of Actions:\\n- **locate('object')** is crucial to correctly identify and target the specified kitchenware, as the robot initially focused on cylinders and spheres.\\n- **pick('object')** ensures that only correct items aligned with our goal are manipulated.\\n- **place_on_top_of('object')** helps ensure items are placed in the intended location or mimic the act of interaction with the dishwasher.\\n- **done()** completes the process once all items are successfully placed.\\n\\nThis revised plan should align robot actions with the task goal and rectify the current challenges.\", '[replan_response]': '[\"locate(\\'plate\\')\", \"pick(\\'plate\\')\", \"locate(\\'dishwasher\\')\", \"place_on_top_of(\\'dishwasher\\')\", \"locate(\\'wineglass\\')\", \"pick(\\'wineglass\\')\", \"locate(\\'dishwasher\\')\", \"place_on_top_of(\\'dishwasher\\')\", \"locate(\\'mug\\')\", \"pick(\\'mug\\')\", \"locate(\\'dishwasher\\')\", \"place_on_top_of(\\'dishwasher\\')\", \"locate(\\'dishbowl\\')\", \"pick(\\'dishbowl\\')\", \"locate(\\'dishwasher\\')\", \"place_on_top_of(\\'dishwasher\\')\", \"done()\"]'}\n"
     ]
    }
   ],
   "source": [
    "status = \"\"\n",
    "wait_time = 13\n",
    "while status != \"completed\":\n",
    "    time.sleep(wait_time)\n",
    "    result_response = handler.get_task_result(model_id=model_id, task_id=task_id)\n",
    "    status = result_response.get(\"status\")\n",
    "    print(f\"Status: {status}\")\n",
    "\n",
    "if status == \"not found\":\n",
    "    raise Exception(\"Task failed: Task not found\")\n",
    "\n",
    "info = result_response.get(\"result\")\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"locate('plate')\", \"pick('plate')\", \"locate('dishwasher')\", \"place_on_top_of('dishwasher')\", \"locate('wineglass')\", \"pick('wineglass')\", \"locate('dishwasher')\", \"place_on_top_of('dishwasher')\", \"locate('mug')\", \"pick('mug')\", \"locate('dishwasher')\", \"place_on_top_of('dishwasher')\", \"locate('dishbowl')\", \"pick('dishbowl')\", \"locate('dishwasher')\", \"place_on_top_of('dishwasher')\", \"done()\"]\n"
     ]
    }
   ],
   "source": [
    "print(info.get(\"[replan_response]\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
