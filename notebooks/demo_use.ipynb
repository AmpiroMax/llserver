{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from llserver.utils.handler import UniserverHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = UniserverHandler(port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler.get_running_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler.start_model(\"lera_api\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = handler.get_running_models()\n",
    "model_id = list(info[\"running_models\"][\"models\"].keys())[0]\n",
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"compare images, describe the difference between them\"\n",
    "base_path = \"/llserver/data/\"\n",
    "image_paths = [\n",
    "    base_path + \"lera_test_replanner/0.png\",\n",
    "    base_path + \"metatmp.png\",\n",
    "]\n",
    "# model = \"gemini-pro-1.5\"\n",
    "model = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "put_response = handler.put_task(model_id=model_id, prompt=prompt, image_paths=image_paths, model=model)\n",
    "task_id = put_response[\"task_id\"][\"task_id\"]\n",
    "task_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = \"\"\n",
    "wait_time = 3\n",
    "while status != \"completed\":\n",
    "    time.sleep(wait_time)\n",
    "    result_response = handler.get_task_result(model_id=model_id, task_id=task_id)\n",
    "    status = result_response.get(\"status\")\n",
    "    print(f\"Status: {status}\")\n",
    "\n",
    "result = result_response.get(\"result\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler.stop_model(model_id=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler.stop_all_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Lera API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLServer and Relanner\n",
    "\n",
    "import os\n",
    "from llserver.utils.handler import UniserverHandler\n",
    "import json\n",
    "import time\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def save_images(images, dirs):\n",
    "    for idx, (image, dir) in enumerate(zip(images, dirs)):\n",
    "        Image.fromarray(image).save(dir)\n",
    "    return dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "instruction = \"Move plates, wineglasses, mugs, dishbowls from the table into the dishwasher and then turn it on.\"\n",
    "suc_actions = [\n",
    "    f'[Walk] <kitchentable> (226)',\n",
    "    f'[Walk] <bench> (227)',\n",
    "#    f'[Walk] <bananas> (1001)',\n",
    "#    f'[Grab] <bananas> (1001)',\n",
    "]\n",
    "next_actions = [\n",
    "#    f'[Walk] <kitchentable> (226)',\n",
    "#    f'[Walk] <bench> (227)',\n",
    "    f'[Walk] <bananas> (1001)',\n",
    "    f'[Grab] <bananas> (1001)',\n",
    "]\n",
    "# image_path = r\"C:\\Users\\prestige\\Desktop\\Work\\virtualhome100\\windows_sim\\Output\\script\\Action_0082_normal.png\"\n",
    "image_path = \"~/work/meta_world/llserver/data/metatmp.png\"\n",
    "images = [np.array(Image.open(image_path))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replanning initialization\n",
    "\n",
    "handler = UniserverHandler(port=8000)\n",
    "start_response = handler.start_model(\"lera_api\")\n",
    "model_id = start_response[\"model_id\"]\n",
    "\n",
    "experiment_name = \"VhomeDebug\"\n",
    "model_name = \"gpt-4o\" #+ \"###\" + \"alfred\"\n",
    "# save_base_dir = f'C:\\\\Users\\\\prestige\\\\Desktop\\\\Work\\\\virtualhome100\\\\llserver\\\\data\\\\{experiment_name}'\n",
    "# load_base_dir = f\"/llserver/data/{experiment_name}\"\n",
    "\n",
    "save_base_dir = f\"~/work/meta_world/llserver/data/{experiment_name}\"\n",
    "load_base_dir = f\"/llserver/data/{experiment_name}\"\n",
    "os.makedirs(save_base_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process input\n",
    "\n",
    "prompt = f\"\"\"{instruction}###{suc_actions}###{next_actions}###\"\"\"\n",
    "\n",
    "dirs_to_save = [f\"{save_base_dir}/{idx}.png\" for idx in range(len(images))]\n",
    "dirs_to_load = [f\"{load_base_dir}/{idx}.png\" for idx in range(len(images))]\n",
    "save_images(images, dirs_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "put_response = handler.put_task(model_id=model_id, prompt=prompt, image_paths=dirs_to_load, model=model_name)\n",
    "task_id = put_response[\"task_id\"][\"task_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = \"\"\n",
    "wait_time = 13\n",
    "while status != \"completed\":\n",
    "    time.sleep(wait_time)\n",
    "    result_response = handler.get_task_result(model_id=model_id, task_id=task_id)\n",
    "    status = result_response.get(\"status\")\n",
    "    print(f\"Status: {status}\")\n",
    "\n",
    "if status == \"not found\":\n",
    "    raise Exception(\"Task failed: Task not found\")\n",
    "\n",
    "info = result_response.get(\"result\")\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(info.get(\"[replan_response]\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
