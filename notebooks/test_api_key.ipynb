{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=\"sk-aitunnel-lRVzfpdyQrVrYJGyxiFkgMMXLSz5P2Oz\", # Ключ из нашего сервиса\n",
    "    base_url=\"https://api.aitunnel.ru/v1/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(prompt, image_paths=[], model=\"gpt-4o\"):\n",
    "    content = [\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": prompt\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    if len(image_paths) > 0:\n",
    "        for image_path in image_paths:\n",
    "            base64_image = encode_image(image_path)\n",
    "            content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}\n",
    "            })\n",
    "    \n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content\n",
    "        }\n",
    "    ]\n",
    "    completion = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        max_tokens=500,\n",
    "        model=model\n",
    "    )\n",
    "\n",
    "    text_outputs = completion.choices[0].message.content\n",
    "    return text_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "kekw\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "def foo(text, model=\"gpt-4o\", **kwargs):\n",
    "    print(text)\n",
    "    print(model)\n",
    "    print(kwargs)\n",
    "\n",
    "foo(\"Hello\", model=\"kekw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оба изображения демонстрируют роботизированную руку, манипулирующую объектами, но в разных условиях и с разными задачами.\n",
      "\n",
      "**Общее:**\n",
      "\n",
      "* **Роботизированный манипулятор:** На обоих изображениях присутствует роботизированная рука, предназначенная для манипулирования объектами. Хоть руки и выглядят немного разными (особенно захват), базовая функция та же.\n",
      "* **Наличие объектов для манипуляции:** На обоих изображениях есть объекты, с которыми взаимодействует робот (шарики, кубики).\n",
      "* **Цель — манипуляция:** Основная цель в обоих случаях — использовать роботизированную руку для перемещения или взаимодействия с объектами определенным образом.\n",
      "\n",
      "**Различия:**\n",
      "\n",
      "* **Среда:** Первое изображение показывает робота, работающего над столом или платформой, похожей на игровую доску.  Второе изображение показывает робота, работающего на плоской черной поверхности, напоминающей коврик. \n",
      "* **Тип объекта:** На первом изображении робот взаимодействует с шариками и одним кубиком. На втором — с кубиками и небольшой миской/контейнером.\n",
      "* **Задача:** На первом изображении, похоже, робот пытается толкнуть шарик(и) в определенное место на \"игровой доске\". На втором изображении задача выглядит как сортировка или перемещение предметов в определенные места на коврике (возможно, в миску).\n",
      "* **Вид робота и захват:**  На первом изображении используется красноватая роботизированная рука с двухпальцевым захватом.  На втором изображении рука серая/серебряная с более сложным, похожим на клешню, захватом.\n",
      "* **Перспектива:** Первое изображение демонстрирует сцену с боку, в то время как второе изображение показывает ее сверху.\n",
      "\n",
      "\n",
      "В целом, оба изображения демонстрируют применение роботизированных манипуляторов для выполнения различных задач, подчёркивая их универсальность и адаптивность.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_path1 = \"/home/mpatratskiy/work/meta_world/llserver/notebooks/metatmp1.png\"\n",
    "image_path2 = \"/home/mpatratskiy/work/pybullet/Pybullet/saved_images/image.png\"\n",
    "\n",
    "prompt = \"Опиши эти изображения, что у них общего и чем они отличаются\"\n",
    "image_paths = [image_path1, image_path2]\n",
    "model = \"gemini-pro-1.5\"\n",
    "\n",
    "answer = predict(prompt, image_paths=image_paths, model=model)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='The image shows a simulated robotic arm interacting with objects in a simulated environment. The arm, primarily red, is positioned over a rectangular wooden surface, possibly a table or platform.\\n\\nHere\\'s a breakdown of the elements:\\n\\n* **Robotic Arm:** The main subject is a multi-jointed robotic arm in a predominantly red color. It appears to be a collaborative robot (cobot) designed for interaction with a shared workspace. The arm is extended over the table, and the end-effector (the \"hand\" of the robot) seems to have a two-fingered gripper or a similar tool attached.\\n* **Table/Platform:** The wooden surface below the robotic arm is set within a metal or plastic frame, creating a container-like structure. This suggests a controlled environment for the robot\\'s tasks.\\n* **Objects:**\\n    * A small red cylinder is on the wooden surface.  It is likely the target object for the robotic arm.\\n    * A small blue sphere is visible near the back edge of the table, against the grey background. its role is uncertain; it could be another target, a reference point for the robot’s navigation, or part of the simulation environment.\\n    * Two short, glowing cyan lines extend downwards from the robot’s end-effector.  These could represent sensors, lasers, or other tools integrated into the gripper. A small red dot at the bottom of each line suggests points of interaction or detection.\\n* **Background:** The background is a uniform grey, typical of simulated environments.  This minimizes distractions and emphasizes the interaction between the robot and the objects.\\n* **Simulation:** The overall appearance strongly suggests a simulated or virtual environment rather than a photograph of a real-world setup. The lighting, textures, and the way objects are rendered contribute to this impression.\\n\\n\\nThe scene likely depicts a simulation used for training the robot, testing algorithms, or designing automated tasks. The robot could be performing pick-and-place operations, object detection, or some other type of manipulation in this virtual workspace.\\n', refusal='', role='assistant', audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "image_path = \"/home/mpatratskiy/work/meta_world/llserver/notebooks/metatmp1.png\"\n",
    "base64_image = encode_image(image_path)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What’s in this image?\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=500, # Старайтесь указывать для более точного расчёта цены\n",
    "    model=\"gemini-pro-1.5\"\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Path to your image\n",
    "image_path = \"path_to_your_image.jpg\"\n",
    "\n",
    "# Getting the base64 string\n",
    "\n",
    "\n",
    "headers = {\n",
    "  \"Content-Type\": \"application/json\",\n",
    "  \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "\n",
    "payload = {\n",
    "  \"model\": \"gpt-4o\",\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"What’s in this image?\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"max_tokens\": 300\n",
    "}\n",
    "\n",
    "response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "369"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "You are a helpful embodied assistant. You are helping with resolving errors in robot execution.\n",
    "During execution of last action you have encountered a problem. Robot failed executing last action.\n",
    "You are given a goal and a current plan and description of the current scene with ideas of what could be wrong.\n",
    "Provde ideas how to fix the problem. HINT: robot may accidently drop objects during gripper movement but continue to move fripper with object.\n",
    "Predict new plan what can fix the problem and reach the goal. Explain each choosen action briefly. You must use actions only from avaliable_actions in you predicted plan. Pay attention on action description. Do not use any other actions and formatting. Do not create combinations of actions.\n",
    "\n",
    "Action description:\n",
    "locate('object') - function that give you position of the object in the scene. Allows to move robot arm to the object in order to pick it up or place_on_top_of another object on it.\n",
    "pick('object') - function that pick up an object. Can be used only if object location is known. Thus you must use locate('object') before pick('object').\n",
    "place_on_top_of('object') - function that put something from gripper on top of the object, which name is provided to function. Example: place_on_top_of('blue block') - put anything from gripper on top of the blue block.\n",
    "done() - function that indicate that the task is completed.\n",
    "Each of locate, pick and place_on_top_of actions takes one argument - object name. You can not use any other arguments or multiple objects.\n",
    "\n",
    "Strong advice: \n",
    "- always use locate('object') before pick('object')\n",
    "- always use locate('object') before place_on_top_of('object')\n",
    "- pick('object') can be used only if gripper is empty and object is located\n",
    "- place_on_top_of('object') can be used only if gripper is NOT empty and object is located\n",
    "\n",
    "Scene description and ideas of what could be wrong:\n",
    "T\n",
    "\n",
    "Avaliable actions:\n",
    "[\"locate('red bowl')\", \"locate('red block')\", \"locate('green block')\", \"place_on_top_of('red bowl')\", \"place_on_top_of('blue block')\", \"place_on_top_of('green block')\", \"pick('blue block')\", \"locate('blue block')\", 'done()', \"pick('red bowl')\", \"pick('red block')\", \"pick('green block')\", \"place_on_top_of('red block')\"]\n",
    "\n",
    "Successfully executed actions:\n",
    "[\"locate('blue block')\", \"pick('blue block')\", \"locate('red bowl')\"]\n",
    "\n",
    "Current plan:\n",
    "[\"place_on_top_of('red bowl')\", \"locate('red block')\", \"pick('red block')\", \"locate('blue block')\", \"place_on_top_of('blue block')\", \"locate('green block')\", \"pick('green block')\", \"locate('red block')\", \"place_on_top_of('red block')\", 'done()']\n",
    "\n",
    "Current goal:\n",
    "stack all blocks on each other in the red bowl\n",
    "\"\"\".split()\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
